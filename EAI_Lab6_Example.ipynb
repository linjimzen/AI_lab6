{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **1.Import Pytorch**"
      ],
      "metadata": {
        "id": "fuGTJZyOs2ec"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "o9p9VvQZspbf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models.feature_extraction as feature_extraction\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import os\n",
        "from torchsummary import summary\n",
        "\n",
        "no_cuda = False\n",
        "use_gpu = not no_cuda and torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_gpu else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2.Load Fashion MNIST Dataset**"
      ],
      "metadata": {
        "id": "OhWHQDwCs75O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "#Dataset\n",
        "train_dataset = torchvision.datasets.FashionMNIST(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset = torchvision.datasets.FashionMNIST(root='./data', train=False, transform=transform, download=True)\n",
        "\n",
        "#Dataloader\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rl04W5rGs7Pc",
        "outputId": "d2d6603f-caa7-434b-810b-ea5a904b0bb5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26421880/26421880 [00:01<00:00, 16731806.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 272797.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4422102/4422102 [00:00<00:00, 5051057.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 7190235.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3.Create a NN model**"
      ],
      "metadata": {
        "id": "RxP37BOvunNX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ToyModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.nn1 = nn.Linear(28*28, 120)\n",
        "    self.nn2 = nn.Linear(120, 84)\n",
        "    self.nn3 = nn.Linear(84, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x.view(-1, 28 * 28)\n",
        "    x = F.relu(self.nn1(x))\n",
        "    x = F.relu(self.nn2(x))\n",
        "    x = self.nn3(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "v7TjEZwbuljD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Print summary of model\n",
        "FP32_model = ToyModel().to(device)\n",
        "summary(FP32_model,(1,28,28))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TkPcH4p9w3d8",
        "outputId": "3b26c0e6-76d6-42e9-c11b-15756a8d20e1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Linear-1                  [-1, 120]          94,200\n",
            "            Linear-2                   [-1, 84]          10,164\n",
            "            Linear-3                   [-1, 10]             850\n",
            "================================================================\n",
            "Total params: 105,214\n",
            "Trainable params: 105,214\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.00\n",
            "Params size (MB): 0.40\n",
            "Estimated Total Size (MB): 0.41\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4.Train model**"
      ],
      "metadata": {
        "id": "zeAVUk-czHsC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 1e-3\n",
        "epochs = 3\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(FP32_model.parameters(), lr=learning_rate, momentum=0.9)\n",
        "\n",
        "FP32_model.to(device) #Put model on GPU"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xXxi_00VzFoa",
        "outputId": "f405451b-fdea-4461-bc1b-9841038e3d39"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ToyModel(\n",
              "  (nn1): Linear(in_features=784, out_features=120, bias=True)\n",
              "  (nn2): Linear(in_features=120, out_features=84, bias=True)\n",
              "  (nn3): Linear(in_features=84, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#train model\n",
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "  size = len(dataloader.dataset)\n",
        "  #Set the model to train mode\n",
        "  model.train()\n",
        "  for batch, (x, y) in enumerate(dataloader):\n",
        "    if use_gpu:\n",
        "      x, y = x.cuda(), y.cuda() #Put data on GPU\n",
        "    optimizer.zero_grad()\n",
        "    #forward\n",
        "    pred = model(x)\n",
        "\n",
        "    #loss\n",
        "    loss = loss_fn(pred, y)\n",
        "\n",
        "    #backward\n",
        "    loss.backward()\n",
        "\n",
        "    #optimize\n",
        "    optimizer.step()\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      loss, current = loss.item(), (batch + 1) * len(x)\n",
        "      print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "  #set model to evaluate mode\n",
        "  model.eval()\n",
        "  size = len(dataloader.dataset)\n",
        "  num_batches = len(dataloader)\n",
        "  test_loss, correct = 0, 0\n",
        "  with torch.no_grad():\n",
        "    for x, y in dataloader:\n",
        "      if use_gpu:\n",
        "        x, y = x.cuda(), y.cuda()\n",
        "      pred = model(x)\n",
        "      test_loss = loss_fn(pred, y).item()\n",
        "      correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "  test_loss /= num_batches\n",
        "  correct /= size\n",
        "  print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ],
      "metadata": {
        "id": "5Q9E2Mdbza4S"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(epochs):\n",
        "  print(f\"Epoch {i+1}\\n-------------------------------\")\n",
        "  train_loop(train_loader, FP32_model, loss_fn, optimizer)\n",
        "  test_loop(test_loader, FP32_model, loss_fn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOGK1V5QzkDT",
        "outputId": "c5172543-beb6-445b-d242-d11b30fbc0e5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.311991  [   32/60000]\n",
            "loss: 2.072658  [ 3232/60000]\n",
            "loss: 1.539363  [ 6432/60000]\n",
            "loss: 1.172183  [ 9632/60000]\n",
            "loss: 1.100624  [12832/60000]\n",
            "loss: 0.840749  [16032/60000]\n",
            "loss: 0.732234  [19232/60000]\n",
            "loss: 0.800273  [22432/60000]\n",
            "loss: 0.615576  [25632/60000]\n",
            "loss: 0.533384  [28832/60000]\n",
            "loss: 0.636276  [32032/60000]\n",
            "loss: 0.561942  [35232/60000]\n",
            "loss: 0.689834  [38432/60000]\n",
            "loss: 0.744495  [41632/60000]\n",
            "loss: 0.396491  [44832/60000]\n",
            "loss: 0.479610  [48032/60000]\n",
            "loss: 0.587342  [51232/60000]\n",
            "loss: 0.659354  [54432/60000]\n",
            "loss: 0.517647  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 80.3%, Avg loss: 0.000919 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.471254  [   32/60000]\n",
            "loss: 0.584544  [ 3232/60000]\n",
            "loss: 0.388880  [ 6432/60000]\n",
            "loss: 0.380573  [ 9632/60000]\n",
            "loss: 0.674940  [12832/60000]\n",
            "loss: 0.532947  [16032/60000]\n",
            "loss: 0.902433  [19232/60000]\n",
            "loss: 0.354916  [22432/60000]\n",
            "loss: 0.645885  [25632/60000]\n",
            "loss: 0.376863  [28832/60000]\n",
            "loss: 0.416570  [32032/60000]\n",
            "loss: 0.597203  [35232/60000]\n",
            "loss: 0.262868  [38432/60000]\n",
            "loss: 0.303285  [41632/60000]\n",
            "loss: 0.528732  [44832/60000]\n",
            "loss: 0.235809  [48032/60000]\n",
            "loss: 0.624632  [51232/60000]\n",
            "loss: 0.889616  [54432/60000]\n",
            "loss: 0.418027  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 82.9%, Avg loss: 0.000900 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.266739  [   32/60000]\n",
            "loss: 0.787983  [ 3232/60000]\n",
            "loss: 0.406980  [ 6432/60000]\n",
            "loss: 0.570383  [ 9632/60000]\n",
            "loss: 0.386589  [12832/60000]\n",
            "loss: 0.398054  [16032/60000]\n",
            "loss: 0.329823  [19232/60000]\n",
            "loss: 0.557519  [22432/60000]\n",
            "loss: 0.382849  [25632/60000]\n",
            "loss: 0.334456  [28832/60000]\n",
            "loss: 0.409461  [32032/60000]\n",
            "loss: 0.412877  [35232/60000]\n",
            "loss: 0.399788  [38432/60000]\n",
            "loss: 0.333823  [41632/60000]\n",
            "loss: 0.404991  [44832/60000]\n",
            "loss: 0.584734  [48032/60000]\n",
            "loss: 0.612239  [51232/60000]\n",
            "loss: 0.351871  [54432/60000]\n",
            "loss: 0.456140  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 83.9%, Avg loss: 0.000793 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5.Post Training Quantization**"
      ],
      "metadata": {
        "id": "nxvaRwTC0EYC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use Pytorch setup -> use some input data to calibrate -> convert to quantize model"
      ],
      "metadata": {
        "id": "RPhhHmRrTjz8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Import quantization\n",
        "from torch.ao.quantization import get_default_qconfig\n",
        "from torch.ao.quantization.quantize_fx import prepare_fx, convert_fx\n",
        "from torch.ao.quantization import QConfigMapping\n",
        "import copy"
      ],
      "metadata": {
        "id": "u8aygP2T0BiJ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = copy.deepcopy(FP32_model) #copy FP32 model\n",
        "model.eval()\n",
        "model.cpu()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_Fm8gB60M80",
        "outputId": "6391f1d8-3684-4128-dfe8-b8ecb0334838"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ToyModel(\n",
              "  (nn1): Linear(in_features=784, out_features=120, bias=True)\n",
              "  (nn2): Linear(in_features=120, out_features=84, bias=True)\n",
              "  (nn3): Linear(in_features=84, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use Pytorch setup"
      ],
      "metadata": {
        "id": "ivYilBTvT2Ed"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#set quantization config\n",
        "qconfig = get_default_qconfig('qnnpack')\n",
        "\n",
        "qconfig_mapping = QConfigMapping().set_global(qconfig)"
      ],
      "metadata": {
        "id": "dWTvsoF20TPB"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "calibrate"
      ],
      "metadata": {
        "id": "rIlhqAhJT5fA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_inputs = (next(iter(train_loader))[0]) #to know model input data type\n",
        "prepared_model = prepare_fx(model, qconfig_mapping, example_inputs)"
      ],
      "metadata": {
        "id": "QVGNBnAF0emP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6f80e3e-406b-4a19-ff24-b126fbec245d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/overrides.py:110: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
            "  torch.has_cuda,\n",
            "/usr/local/lib/python3.10/dist-packages/torch/overrides.py:111: UserWarning: 'has_cudnn' is deprecated, please use 'torch.backends.cudnn.is_available()'\n",
            "  torch.has_cudnn,\n",
            "/usr/local/lib/python3.10/dist-packages/torch/overrides.py:117: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
            "  torch.has_mps,\n",
            "/usr/local/lib/python3.10/dist-packages/torch/overrides.py:118: UserWarning: 'has_mkldnn' is deprecated, please use 'torch.backends.mkldnn.is_available()'\n",
            "  torch.has_mkldnn,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calibrate(model, device, data_loader):\n",
        "  model.to(device)\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for x, y in data_loader:\n",
        "      x, y = x.to(device), y.to(device) #device\n",
        "      model(x)\n",
        "calibrate(prepared_model, 'cpu', test_loader)"
      ],
      "metadata": {
        "id": "023Z64960l1S"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "convert to quantized model"
      ],
      "metadata": {
        "id": "e6Tr42LeT7hv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PTQ_model = convert_fx(prepared_model)"
      ],
      "metadata": {
        "id": "FBZbw2V50tpF"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "check quantized model"
      ],
      "metadata": {
        "id": "iEgPtJEK0yGm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(PTQ_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1httbLAH0vy_",
        "outputId": "3c2ddf14-1566-454b-8711-b6d79429c779"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GraphModule(\n",
            "  (nn1): QuantizedLinearReLU(in_features=784, out_features=120, scale=0.02781897969543934, zero_point=0, qscheme=torch.per_tensor_affine)\n",
            "  (nn2): QuantizedLinearReLU(in_features=120, out_features=84, scale=0.03556470572948456, zero_point=0, qscheme=torch.per_tensor_affine)\n",
            "  (nn3): QuantizedLinear(in_features=84, out_features=10, scale=0.1019989475607872, zero_point=113, qscheme=torch.per_tensor_affine)\n",
            ")\n",
            "\n",
            "\n",
            "\n",
            "def forward(self, x):\n",
            "    _input_scale_0 = self._input_scale_0\n",
            "    _input_zero_point_0 = self._input_zero_point_0\n",
            "    quantize_per_tensor = torch.quantize_per_tensor(x, _input_scale_0, _input_zero_point_0, torch.quint8);  x = _input_scale_0 = _input_zero_point_0 = None\n",
            "    view = quantize_per_tensor.view(-1, 784);  quantize_per_tensor = None\n",
            "    nn1 = self.nn1(view);  view = None\n",
            "    nn2 = self.nn2(nn1);  nn1 = None\n",
            "    nn3 = self.nn3(nn2);  nn2 = None\n",
            "    dequantize_4 = nn3.dequantize();  nn3 = None\n",
            "    return dequantize_4\n",
            "    \n",
            "# To see more debug info, please use `graph_module.print_readable()`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **6.Quantization Aware Training**"
      ],
      "metadata": {
        "id": "JSVnY_yG14od"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use Pytorch setup -> use input data to fine-tune model with fake quantize layer -> convert to quantize model"
      ],
      "metadata": {
        "id": "FRYOXSavUHcS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = copy.deepcopy(FP32_model)"
      ],
      "metadata": {
        "id": "YZZ-urYH1739"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use Pytorch setup"
      ],
      "metadata": {
        "id": "YIeBIDu-UaCP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.qconfig = torch.ao.quantization.get_default_qconfig('qnnpack')\n",
        "qconfig_mapping = QConfigMapping().set_global(qconfig)\n",
        "\n",
        "example_inputs = (next(iter(train_loader))[0]) #to know model input data type\n",
        "prepared_model = torch.ao.quantization.quantize_fx.prepare_qat_fx(model, qconfig_mapping, example_inputs) # prepare to quantize model (fuse module (ex:CONV+BN+RELU...)，insert observer)\n",
        "prepared_model.train()\n",
        "prepared_model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9WAFSdW194d",
        "outputId": "2bba08b0-4da8-445a-e7db-df3a914c8459"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GraphModule(\n",
              "  (activation_post_process_0): HistogramObserver(min_val=inf, max_val=-inf)\n",
              "  (activation_post_process_1): HistogramObserver(min_val=inf, max_val=-inf)\n",
              "  (nn1): LinearReLU(\n",
              "    in_features=784, out_features=120, bias=True\n",
              "    (weight_fake_quant): MinMaxObserver(min_val=inf, max_val=-inf)\n",
              "  )\n",
              "  (activation_post_process_2): HistogramObserver(min_val=inf, max_val=-inf)\n",
              "  (nn2): LinearReLU(\n",
              "    in_features=120, out_features=84, bias=True\n",
              "    (weight_fake_quant): MinMaxObserver(min_val=inf, max_val=-inf)\n",
              "  )\n",
              "  (activation_post_process_3): HistogramObserver(min_val=inf, max_val=-inf)\n",
              "  (nn3): Linear(\n",
              "    in_features=84, out_features=10, bias=True\n",
              "    (weight_fake_quant): MinMaxObserver(min_val=inf, max_val=-inf)\n",
              "  )\n",
              "  (activation_post_process_4): HistogramObserver(min_val=inf, max_val=-inf)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training fake quantize model"
      ],
      "metadata": {
        "id": "aK7wg5fkUcNj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 1e-3\n",
        "epochs = 1\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(prepared_model.parameters(), lr=learning_rate, momentum=0.9)"
      ],
      "metadata": {
        "id": "zwAYQ2mw2FnL"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "  print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
        "  train_loop(train_loader, prepared_model, loss_fn, optimizer)\n",
        "  test_loop(test_loader, prepared_model, loss_fn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AipJ-cTa2Jbg",
        "outputId": "4c7d872d-d898-46ba-8ce1-1e111fe68fe7"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 0.440943  [   32/60000]\n",
            "loss: 0.425902  [ 3232/60000]\n",
            "loss: 0.624886  [ 6432/60000]\n",
            "loss: 0.464961  [ 9632/60000]\n",
            "loss: 0.463102  [12832/60000]\n",
            "loss: 0.327730  [16032/60000]\n",
            "loss: 0.335214  [19232/60000]\n",
            "loss: 0.374603  [22432/60000]\n",
            "loss: 0.446798  [25632/60000]\n",
            "loss: 0.401708  [28832/60000]\n",
            "loss: 0.564758  [32032/60000]\n",
            "loss: 0.600982  [35232/60000]\n",
            "loss: 0.378758  [38432/60000]\n",
            "loss: 0.471951  [41632/60000]\n",
            "loss: 0.425112  [44832/60000]\n",
            "loss: 0.413184  [48032/60000]\n",
            "loss: 0.594654  [51232/60000]\n",
            "loss: 0.248248  [54432/60000]\n",
            "loss: 0.406001  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 84.6%, Avg loss: 0.000642 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prepared_model.cpu()\n",
        "prepared_model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vh_DbulY2gCc",
        "outputId": "9235743d-a089-4716-fa40-d217cc4694c3"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GraphModule(\n",
              "  (activation_post_process_0): HistogramObserver(min_val=-1.0, max_val=1.0)\n",
              "  (activation_post_process_1): HistogramObserver(min_val=-1.0, max_val=1.0)\n",
              "  (nn1): LinearReLU(\n",
              "    in_features=784, out_features=120, bias=True\n",
              "    (weight_fake_quant): MinMaxObserver(min_val=-0.1149161159992218, max_val=0.11076779663562775)\n",
              "  )\n",
              "  (activation_post_process_2): HistogramObserver(min_val=0.0, max_val=7.873921871185303)\n",
              "  (nn2): LinearReLU(\n",
              "    in_features=120, out_features=84, bias=True\n",
              "    (weight_fake_quant): MinMaxObserver(min_val=-0.19614410400390625, max_val=0.22086060047149658)\n",
              "  )\n",
              "  (activation_post_process_3): HistogramObserver(min_val=0.0, max_val=10.523351669311523)\n",
              "  (nn3): Linear(\n",
              "    in_features=84, out_features=10, bias=True\n",
              "    (weight_fake_quant): MinMaxObserver(min_val=-0.3988783359527588, max_val=0.4632803201675415)\n",
              "  )\n",
              "  (activation_post_process_4): HistogramObserver(min_val=-13.051665306091309, max_val=15.669031143188477)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "convert to quantized model"
      ],
      "metadata": {
        "id": "ufYTZ1S6UgU9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "QAT_model = convert_fx(prepared_model) # convert the calibrated model to a quantized model"
      ],
      "metadata": {
        "id": "pD5BwI5Z2ibz"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(QAT_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wuVI0ngY2nQx",
        "outputId": "63c2801c-3c61-4604-c8f3-6bed1e706e62"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GraphModule(\n",
            "  (nn1): QuantizedLinearReLU(in_features=784, out_features=120, scale=0.027018360793590546, zero_point=0, qscheme=torch.per_tensor_affine)\n",
            "  (nn2): QuantizedLinearReLU(in_features=120, out_features=84, scale=0.035908035933971405, zero_point=0, qscheme=torch.per_tensor_affine)\n",
            "  (nn3): QuantizedLinear(in_features=84, out_features=10, scale=0.1013011708855629, zero_point=110, qscheme=torch.per_tensor_affine)\n",
            ")\n",
            "\n",
            "\n",
            "\n",
            "def forward(self, x):\n",
            "    _input_scale_0 = self._input_scale_0\n",
            "    _input_zero_point_0 = self._input_zero_point_0\n",
            "    quantize_per_tensor = torch.quantize_per_tensor(x, _input_scale_0, _input_zero_point_0, torch.quint8);  x = _input_scale_0 = _input_zero_point_0 = None\n",
            "    view = quantize_per_tensor.view(-1, 784);  quantize_per_tensor = None\n",
            "    nn1 = self.nn1(view);  view = None\n",
            "    nn2 = self.nn2(nn1);  nn1 = None\n",
            "    nn3 = self.nn3(nn2);  nn2 = None\n",
            "    dequantize_4 = nn3.dequantize();  nn3 = None\n",
            "    return dequantize_4\n",
            "    \n",
            "# To see more debug info, please use `graph_module.print_readable()`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **7.Compare FP32、PTQ and QAT model**"
      ],
      "metadata": {
        "id": "8zvNOKGs2rxZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_size_of_model(model):\n",
        "    \"\"\" Print the size of the model.\n",
        "\n",
        "    Args:\n",
        "        model: model whose size needs to be determined\n",
        "\n",
        "    \"\"\"\n",
        "    torch.save(model.state_dict(), \"temp.p\")\n",
        "    print('Size of the model(MB):', os.path.getsize(\"temp.p\")/1e6)\n",
        "    os.remove('temp.p')\n",
        "\n",
        "def compare(model, device, test_loader, quantize=False):\n",
        "  model.to(device)\n",
        "  model.eval()\n",
        "\n",
        "  total = 0\n",
        "  correct = 0\n",
        "  with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "      images, labels = data\n",
        "      images, labels = images.to(device),labels.to(device)\n",
        "      outputs = model(images)\n",
        "      # the class with the highest energy is what we choose as prediction\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      total += labels.size(0)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "\n",
        "  test_loss = 0\n",
        "\n",
        "  print(\"========================================= PERFORMANCE =============================================\")\n",
        "  print_size_of_model(model)\n",
        "  print('\\nAccuracy: {}/{} ({:.0f}%)\\n'.format( correct, total,100. * correct / total))"
      ],
      "metadata": {
        "id": "-rBQ2eid2py_"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compare(model=FP32_model, device=\"cpu\", test_loader=test_loader)"
      ],
      "metadata": {
        "id": "yA0KjHG72xJl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compare(model=PTQ_model, device=\"cpu\", test_loader=test_loader)"
      ],
      "metadata": {
        "id": "SNksXfbP2z-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compare(model=QAT_model, device=\"cpu\", test_loader=test_loader)"
      ],
      "metadata": {
        "id": "S7mkJ7-U23TI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **8.Quantize yourself**"
      ],
      "metadata": {
        "id": "57cEwOIA26_w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8.1 Quantize layer by layer"
      ],
      "metadata": {
        "id": "4L-awfSh-SGs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = copy.deepcopy(FP32_model).to(\"cpu\")\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7I8VxP-ksnEl",
        "outputId": "f1e360b9-d4fa-4379-edfe-886da57890a2"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ToyModel(\n",
            "  (nn1): Linear(in_features=784, out_features=120, bias=True)\n",
            "  (nn2): Linear(in_features=120, out_features=84, bias=True)\n",
            "  (nn3): Linear(in_features=84, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class QuantizedLinear(nn.Module):\n",
        "  def __init__(self, in_features, out_features, weight, bias):\n",
        "    super(QuantizedLinear, self).__init__()\n",
        "    self.in_features = in_features\n",
        "    self.out_features = out_features\n",
        "    self.scale, self.zero_point = None, None\n",
        "    self.weight = self._weight_quantize(weight)\n",
        "    self.bias = bias\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = torch.ops.quantized.matmul(x, self.weight.t(), self.scale, self.zero_point).dequantize() + self.bias\n",
        "    output = torch.quantize_per_tensor(x, self.scale, self.zero_point, dtype=torch.qint8)\n",
        "    return output\n",
        "\n",
        "  def _weight_quantize(self, weight):\n",
        "    q_min, q_max = -128, 127\n",
        "    min_val, max_val = np.min(weight.detach().numpy()), np.max(weight.detach().numpy())\n",
        "\n",
        "    s = (max_val - min_val) / (q_max - q_min)\n",
        "    z = round(q_min - min_val / s)\n",
        "    return torch.quantize_per_tensor(weight, s, z, dtype=torch.qint8)\n",
        "\n",
        "  def _calibrate(self, x):\n",
        "    x = x.dequantize()\n",
        "    output = torch.matmul(x, self.weight.t().dequantize())\n",
        "    q_min, q_max = -128, 127\n",
        "    min_val, max_val = np.min(output.detach().numpy()), np.max(output.detach().numpy())\n",
        "    self.scale = (max_val - min_val) / (q_max - q_min)\n",
        "    self.zero_point = round(q_min - min_val / self.scale)\n",
        "\n",
        "\n",
        "  def __repr__(self):\n",
        "    return f'QuantizedLinear(in_features={self.in_features}, out_features={self.out_features}, scale={self.scale}, zero_point={self.zero_point})'\n",
        "\n",
        "class QuantizedLinearReLU(nn.Module):\n",
        "  def __init__(self, in_features, out_features, weight, bias):\n",
        "    super(QuantizedLinearReLU, self).__init__()\n",
        "    self.in_features = in_features\n",
        "    self.out_features = out_features\n",
        "    self.scale, self.zero_point = None, None\n",
        "    self.weight = self._weight_quantize(weight)\n",
        "    self.bias = bias\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = torch.ops.quantized.matmul(x, self.weight.t(), self.scale, self.zero_point).dequantize() + self.bias\n",
        "    output = torch.quantize_per_tensor(x, self.scale, self.zero_point, dtype=torch.qint8)\n",
        "    output = F.relu(output)\n",
        "    return output\n",
        "\n",
        "  def _weight_quantize(self, weight):\n",
        "    q_min, q_max = -128, 127\n",
        "    min_val, max_val = np.min(weight.detach().numpy()), np.max(weight.detach().numpy())\n",
        "\n",
        "    s = (max_val - min_val) / (q_max - q_min)\n",
        "    z = round(q_min - min_val / s)\n",
        "    return torch.quantize_per_tensor(weight, s, z, dtype=torch.qint8)\n",
        "\n",
        "  def _calibrate(self, x):\n",
        "    x = x.dequantize()\n",
        "    output = F.relu(torch.matmul(x, self.weight.t().dequantize()))\n",
        "    q_min, q_max = -128, 127\n",
        "    min_val, max_val = np.min(output.detach().numpy()), np.max(output.detach().numpy())\n",
        "    self.scale = (max_val - min_val) / (q_max - q_min)\n",
        "    self.zero_point = round(q_min - min_val / self.scale)\n",
        "\n",
        "  def __repr__(self):\n",
        "    return f'QuantizedLinearReLU(in_features={self.in_features}, out_features={self.out_features}, scale={self.scale}, zero_point={self.zero_point})'"
      ],
      "metadata": {
        "id": "LVsOJEukoOyp"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class QuantizedModel(nn.Module):\n",
        "  def __init__(self, model):\n",
        "    super(QuantizedModel, self).__init__()\n",
        "    self.weight_dic = []\n",
        "    self.bias_dic = []\n",
        "    self.scale, self.zero_point = None, None  #scale and zero point of input layer\n",
        "    self._get_weight()\n",
        "    self.nn1 = QuantizedLinearReLU(in_features=28*28, out_features=120, weight=self.weight_dic[0], bias=self.bias_dic[0])\n",
        "    self.nn2 = QuantizedLinearReLU(in_features=120, out_features=84, weight=self.weight_dic[1], bias=self.bias_dic[1])\n",
        "    self.nn3 = QuantizedLinear(in_features=84, out_features=10, weight=self.weight_dic[2], bias=self.bias_dic[2])\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x.view(-1, 28 * 28)\n",
        "    x = torch.quantize_per_tensor(x, self.scale, self.zero_point, dtype=torch.qint8)\n",
        "    x = self.nn1(x)\n",
        "    x = self.nn2(x)\n",
        "    x = self.nn3(x)\n",
        "    x = x.dequantize()\n",
        "    return x\n",
        "\n",
        "  def _get_weight(self):\n",
        "    for name, paras in model.named_parameters():\n",
        "      if \"weight\" in name:\n",
        "        self.weight_dic.append(paras)\n",
        "      elif \"bias\" in name:\n",
        "        self.bias_dic.append(paras)\n",
        "\n",
        "  def _calibrate(self, input):\n",
        "    self.scale = (np.max(input.detach().numpy()) - np.min(input.detach().numpy())) / 256\n",
        "    self.zero_point = round(np.min(input.detach().numpy())/self.scale)\n",
        "    input = input.view(-1, 28*28)\n",
        "    input = torch.quantize_per_tensor(input, self.scale, self.zero_point, dtype=torch.qint8)\n",
        "\n",
        "    self.nn1._calibrate(input)\n",
        "    input = self.nn1(input)\n",
        "\n",
        "    self.nn2._calibrate(input)\n",
        "    input = self.nn2(input)\n",
        "\n",
        "    self.nn3._calibrate(input)"
      ],
      "metadata": {
        "id": "WffWu2lBpqb6"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_model = QuantizedModel(model)\n",
        "print(test_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yeqe27PcBKJ6",
        "outputId": "eb8d5b8c-f3c4-4f5a-baae-13ccc5862ce8"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "QuantizedModel(\n",
            "  (nn1): QuantizedLinearReLU(in_features=784, out_features=120, scale=None, zero_point=None)\n",
            "  (nn2): QuantizedLinearReLU(in_features=120, out_features=84, scale=None, zero_point=None)\n",
            "  (nn3): QuantizedLinear(in_features=84, out_features=10, scale=None, zero_point=None)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "這邊需要注意的是，weight的s、z與activation的s、z是分開的，因此每個layer會有兩組(s, z)"
      ],
      "metadata": {
        "id": "7LDvPApLA3Ms"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Calibrate to compute scale and zero point of model\n",
        "for batch in train_loader:\n",
        "  input, label = batch\n",
        "  test_model._calibrate(input)\n",
        "  break\n",
        "print(test_model)"
      ],
      "metadata": {
        "id": "EUEUe7wiqUhh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e68e176f-2bf7-4dff-dc1a-6ba7c2164f94"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "QuantizedModel(\n",
            "  (nn1): QuantizedLinearReLU(in_features=784, out_features=120, scale=0.009455646253099628, zero_point=-128)\n",
            "  (nn2): QuantizedLinearReLU(in_features=120, out_features=84, scale=0.007616113213931813, zero_point=-128)\n",
            "  (nn3): QuantizedLinear(in_features=84, out_features=10, scale=0.02165715834673713, zero_point=-15)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8.2 Quantize all layer at the same time"
      ],
      "metadata": {
        "id": "e7ByIfDoBuWV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = copy.deepcopy(FP32_model).to(\"cpu\")\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8Fr66GAGljY",
        "outputId": "4e258d1e-88ef-4089-aa25-63d25c424ef5"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ToyModel(\n",
            "  (nn1): Linear(in_features=784, out_features=120, bias=True)\n",
            "  (nn2): Linear(in_features=120, out_features=84, bias=True)\n",
            "  (nn3): Linear(in_features=84, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Check name of all layer\n",
        "train_nodes, eval_nodes = feature_extraction.get_graph_node_names(model)\n",
        "print(train_nodes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JgJZOzAkHnQ0",
        "outputId": "84e90fea-cfd7-4637-e1bf-43f9285f30dc"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['x', 'view', 'nn1', 'relu', 'nn2', 'relu_1', 'nn3']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scale_dic = []\n",
        "zero_dic = []\n",
        "\n",
        "#Calibrate to compute s、z of all layer at the same time\n",
        "for batch in train_loader:\n",
        "  input, label = batch\n",
        "  for node in train_nodes:\n",
        "    if node == \"x\" or (\"relu\" in node) or node == \"nn3\":\n",
        "      extractor = feature_extraction.create_feature_extractor(model, [node]).cpu()\n",
        "      output = extractor(input)[node]\n",
        "      q_min, q_max = -128, 127\n",
        "      min_val, max_val = np.min(output.detach().numpy()), np.max(output.detach().numpy())\n",
        "      scale = (max_val - min_val) / (q_max - q_min)\n",
        "      zero = round(q_min - min_val / scale)\n",
        "      scale_dic.append(scale)\n",
        "      zero_dic.append(zero)\n",
        "  break\n",
        "\n",
        "\n",
        "print(scale_dic)\n",
        "print(zero_dic)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aLCRuwUlGwKW",
        "outputId": "f9c88f18-9c46-464e-c938-5e42c6ca9cc5"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.00784313725490196, 0.02774794522453757, 0.036900086496390545, 0.0955727969898897]\n",
            "[0, -128, -128, -23]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class QuantizedLinear2(nn.Module):\n",
        "  def __init__(self, in_features, out_features, weight, bias, scale, zero_point):\n",
        "    super(QuantizedLinear2, self).__init__()\n",
        "    self.in_features = in_features\n",
        "    self.out_features = out_features\n",
        "    self.scale, self.zero_point = scale, zero_point\n",
        "    self.weight = self._weight_quantize(weight)\n",
        "    self.bias = bias\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = torch.ops.quantized.matmul(x, self.weight.t(), self.scale, self.zero_point).dequantize() + self.bias\n",
        "    output = torch.quantize_per_tensor(x, self.scale, self.zero_point, dtype=torch.qint8)\n",
        "\n",
        "    return output\n",
        "\n",
        "  def _weight_quantize(self, weight):\n",
        "    q_min, q_max = -128, 127\n",
        "    min_val, max_val = np.min(weight.detach().numpy()), np.max(weight.detach().numpy())\n",
        "\n",
        "    s = (max_val - min_val) / (q_max - q_min)\n",
        "    z = round(q_min - min_val / s)\n",
        "    return torch.quantize_per_tensor(weight, s, z, dtype=torch.qint8)\n",
        "\n",
        "  def __repr__(self):\n",
        "    return f'QuantizedLinear(in_features={self.in_features}, out_features={self.out_features}, scale={self.scale}, zero_point={self.zero_point})'\n",
        "\n",
        "class QuantizedLinearReLU2(nn.Module):\n",
        "  def __init__(self, in_features, out_features, weight, bias, scale, zero_point):\n",
        "    super(QuantizedLinearReLU2, self).__init__()\n",
        "    self.in_features = in_features\n",
        "    self.out_features = out_features\n",
        "    self.scale, self.zero_point = scale, zero_point\n",
        "    self.weight = self._weight_quantize(weight)\n",
        "    self.bias = bias\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = torch.ops.quantized.matmul(x, self.weight.t(), self.scale, self.zero_point).dequantize() + self.bias\n",
        "    output = torch.quantize_per_tensor(x, self.scale, self.zero_point, dtype=torch.qint8)\n",
        "    output = F.relu(output)\n",
        "\n",
        "    return output\n",
        "\n",
        "  def _weight_quantize(self, weight):\n",
        "    q_min, q_max = -128, 127\n",
        "    min_val, max_val = np.min(weight.detach().numpy()), np.max(weight.detach().numpy())\n",
        "\n",
        "    s = (max_val - min_val) / (q_max - q_min)\n",
        "    z = round(q_min - min_val / s)\n",
        "    return torch.quantize_per_tensor(weight, s, z, dtype=torch.qint8)\n",
        "\n",
        "  def __repr__(self):\n",
        "    return f'QuantizedLinearReLU(in_features={self.in_features}, out_features={self.out_features}, scale={self.scale}, zero_point={self.zero_point})'"
      ],
      "metadata": {
        "id": "ZMH0aVyMF2OW"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class QuantizedModel2(nn.Module):\n",
        "  def __init__(self, model, scale, zero_point):\n",
        "    super(QuantizedModel2, self).__init__()\n",
        "    self.weight_dic = []\n",
        "    self.bias_dic = []\n",
        "    self.scale, self.zero_point = scale, zero_point #scale and zero point of input layer\n",
        "\n",
        "    self._get_weight()\n",
        "    self.nn1 = QuantizedLinearReLU2(in_features=28*28, out_features=120, weight=self.weight_dic[0], bias=self.bias_dic[0], scale=self.scale[1], zero_point=self.zero_point[1])\n",
        "    self.nn2 = QuantizedLinearReLU2(in_features=120, out_features=84, weight=self.weight_dic[1], bias=self.bias_dic[1], scale=self.scale[2], zero_point=self.zero_point[2])\n",
        "    self.nn3 = QuantizedLinear2(in_features=84, out_features=10, weight=self.weight_dic[2], bias=self.bias_dic[2], scale=self.scale[3], zero_point=self.zero_point[3])\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x.view(-1, 28 * 28)\n",
        "    x = torch.quantize_per_tensor(x, self.scale[0], self.zero_point[0], dtype=torch.qint8)\n",
        "    x = self.nn1(x)\n",
        "    x = self.nn2(x)\n",
        "    x = self.nn3(x)\n",
        "    x = x.dequantize()\n",
        "    return x\n",
        "\n",
        "  def _get_weight(self):\n",
        "    for name, paras in model.named_parameters():\n",
        "      if \"weight\" in name:\n",
        "        self.weight_dic.append(paras)\n",
        "      elif \"bias\" in name:\n",
        "        self.bias_dic.append(paras)"
      ],
      "metadata": {
        "id": "xg4cSFQ-GZak"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_model2 = QuantizedModel2(model, scale=scale_dic, zero_point=zero_dic)\n",
        "\n",
        "print(test_model2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PtnJaLCjKAHW",
        "outputId": "d5633ff4-3cd5-4860-9a29-d8f8f3a1df30"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "QuantizedModel2(\n",
            "  (nn1): QuantizedLinearReLU(in_features=784, out_features=120, scale=0.02774794522453757, zero_point=-128)\n",
            "  (nn2): QuantizedLinearReLU(in_features=120, out_features=84, scale=0.036900086496390545, zero_point=-128)\n",
            "  (nn3): QuantizedLinear(in_features=84, out_features=10, scale=0.0955727969898897, zero_point=-23)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8.3 Compare"
      ],
      "metadata": {
        "id": "NHXqakhEDlns"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FP32_model.to(\"cpu\")\n",
        "for batch in train_loader:\n",
        "  input, label = batch\n",
        "  output1 = FP32_model(input)[0]    #FP32 model\n",
        "  output2 = test_model(input)[0]    #Quantize layer by layer\n",
        "  output3 = test_model2(input)[0]    #Quantize at the same time\n",
        "  print(output1)\n",
        "  print(output2)\n",
        "  print(output3)\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3WuAjf-hFdRv",
        "outputId": "0455c3f5-07e2-41b7-bc47-5a4481c25c41"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-3.1556, -0.4147, -2.7360, -3.8460, -2.4293,  4.8535, -3.8508,  9.1655,\n",
            "         1.3757,  0.6913], grad_fn=<SelectBackward0>)\n",
            "tensor([-0.1299, -0.0217, -0.3465, -0.4115, -0.4548,  0.1949, -0.5631,  0.8879,\n",
            "         0.2599,  0.5198])\n",
            "tensor([-3.1539, -0.3823, -2.7716, -3.8229, -2.4849,  4.8742, -3.7273,  8.9838,\n",
            "         1.3380,  0.6690])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Quantize layer by layer\n",
        "total = 0\n",
        "correct = 0\n",
        "with torch.no_grad():\n",
        "  for data in test_loader:\n",
        "    images, labels = data\n",
        "    outputs = test_model(images)\n",
        "    # the class with the highest energy is what we choose as prediction\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "\n",
        "test_loss = 0\n",
        "\n",
        "print(\"========================================= PERFORMANCE =============================================\")\n",
        "print('\\nAccuracy: {}/{} ({:.0f}%)\\n'.format( correct, total,100. * correct / total))"
      ],
      "metadata": {
        "id": "ybf4ARuaDsuQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Quantize at the same time\n",
        "total = 0\n",
        "correct = 0\n",
        "with torch.no_grad():\n",
        "  for data in test_loader:\n",
        "    images, labels = data\n",
        "    outputs = test_model2(images)\n",
        "    # the class with the highest energy is what we choose as prediction\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "\n",
        "test_loss = 0\n",
        "\n",
        "print(\"========================================= PERFORMANCE =============================================\")\n",
        "print('\\nAccuracy: {}/{} ({:.0f}%)\\n'.format( correct, total,100. * correct / total))"
      ],
      "metadata": {
        "id": "WMA7griAFu7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8.4 Compute MSE of output of each layer"
      ],
      "metadata": {
        "id": "Il7XmKXiErg6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_nodes, eval_nodes = feature_extraction.get_graph_node_names(test_model)\n",
        "print(train_nodes)\n",
        "train_nodes, eval_nodes = feature_extraction.get_graph_node_names(test_model2)\n",
        "print(train_nodes)"
      ],
      "metadata": {
        "id": "hx-Ij35XoBrx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Compare MSE of 2 different methods\n",
        "\n",
        "for batch in train_loader:\n",
        "  input, label = batch\n",
        "  for train_node in train_nodes:\n",
        "    if train_node == \"quantize_per_tensor\" or \"relu\" in train_node or train_node == \"dequantize\":\n",
        "      extractor1 = feature_extraction.create_feature_extractor(test_model, [train_node]).cpu()\n",
        "      output1 = extractor1(input)[train_node]\n",
        "      extractor2 = feature_extraction.create_feature_extractor(test_model2, [train_node]).cpu()\n",
        "      output2 = extractor2(input)[train_node]\n",
        "      mse = F.mse_loss(output1.dequantize(), output2.dequantize())\n",
        "      print(f'MSE of layer {train_node} is {mse}')\n",
        "  break"
      ],
      "metadata": {
        "id": "Plwghz87n5DL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}